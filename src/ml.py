# -*- coding: utf-8 -*-
"""ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PT6cqyEGFZ6rArfoBIu2EFQ2HxnsAO6j
"""

import pandas as pd
#https://4geeks.com/syllabus/spain-ds-pt-3/read/exploratory-data-analysis-and-cleaning-with-titanic
#https://4geeks.com/syllabus/spain-ds-pt-3/project/logistic-regression-project-tutorial

url = "https://raw.githubusercontent.com/4GeeksAcademy/logistic-regression-project-tutorial/main/bank-marketing-campaign-data.csv"
df = pd.read_csv(url)

df = pd.read_csv(url, sep=';')

df.info()
df.shape

print(df.isnull().sum())

print(df.duplicated().sum())

duplicates = df[df.duplicated()]

# Mostrar las filas duplicadas
print("Filas duplicadas:")
print(duplicates)

duplicates_count = df[df.duplicated(keep=False)].value_counts()

# Mostrar el conteo de duplicados
print("Conteo de valores duplicados:")
print(duplicates_count)

df.drop_duplicates(inplace=True)

df.shape

df.drop(["contact", "month", "day_of_week", "pdays", "previous", "nr.employed"], axis = 1, inplace = True)

df.info()

"""Variables categoricas"""

import matplotlib.pyplot as plt
import seaborn as sns

# Crear un objeto figura y ejes con subgráficos 2x4
fig, axes = plt.subplots(2, 4, figsize=(20, 10))

# Crear histogramas para cada variable
sns.histplot(data=df, x="age", ax=axes[0, 0]).set_xlim(0, 99)
sns.countplot(data=df, x="job", ax=axes[0, 1]).set(ylabel=None)
sns.countplot(data=df, x="marital", ax=axes[0, 2]).set(ylabel=None)
sns.countplot(data=df, x="education", ax=axes[0, 3]).set(ylabel=None)
sns.countplot(data=df, x="default", ax=axes[1, 0]).set(ylabel=None)
sns.countplot(data=df, x="housing", ax=axes[1, 1]).set(ylabel=None)

# Añadir las variables 'loan' y 'poutcome'
sns.countplot(data=df, x="loan", ax=axes[1, 2]).set(ylabel=None)
sns.countplot(data=df, x="poutcome", ax=axes[1, 3]).set(ylabel=None)

# Ajustar el diseño
plt.tight_layout()

# Mostrar el gráfico
plt.show()

"""Variables numericas"""

import matplotlib.pyplot as plt
import seaborn as sns

# Crear un objeto figura y ejes con subgráficos 4x4
fig, axes = plt.subplots(4, 4, figsize=(10, 7))

# Crear histogramas y boxplots para las variables numéricas
sns.histplot(ax=axes[0, 0], data=df, x="age").set(xlabel=None)
sns.boxplot(ax=axes[1, 0], data=df, x="age")
sns.histplot(ax=axes[0, 1], data=df, x="campaign").set(xlabel=None, ylabel=None)
sns.boxplot(ax=axes[1, 1], data=df, x="campaign")
sns.histplot(ax=axes[0, 2], data=df, x="emp.var.rate").set(xlabel=None, ylabel=None)
sns.boxplot(ax=axes[1, 2], data=df, x="emp.var.rate")
sns.histplot(ax=axes[0, 3], data=df, x="cons.price.idx").set(xlabel=None)
sns.boxplot(ax=axes[1, 3], data=df, x="cons.price.idx")

sns.histplot(ax=axes[2, 0], data=df, x="cons.conf.idx").set(xlabel=None, ylabel=None)
sns.boxplot(ax=axes[3, 0], data=df, x="cons.conf.idx")
sns.histplot(ax=axes[2, 1], data=df, x="euribor3m").set(xlabel=None, ylabel=None)
sns.boxplot(ax=axes[3, 1], data=df, x="euribor3m")
sns.histplot(ax=axes[2, 2], data=df, x="duration").set(xlabel=None, ylabel=None)
sns.boxplot(ax=axes[3, 2], data=df, x="duration")

# Ajustar el diseño
plt.tight_layout()

# Mostrar el gráfico
plt.show()



"""multivariables"""

import matplotlib.pyplot as plt
import seaborn as sns

fig, axis = plt.subplots(2, 2, figsize=(10, 7))

# Crear un diagrama de dispersión
sns.regplot(ax=axis[0, 0], data=df, x="age", y="campaign")
sns.heatmap(df[["age", "campaign"]].corr(), annot=True, fmt=".2f", ax=axis[1, 0], cbar=False)

# Puedes reemplazar "emp.var.rate" y "cons.price.idx" con las variables de interés
sns.regplot(ax=axis[0, 1], data=df, x="emp.var.rate", y="cons.price.idx").set(ylabel=None)
sns.heatmap(df[["emp.var.rate", "cons.price.idx"]].corr(), annot=True, fmt=".2f", ax=axis[1, 1])

# Ajustar el diseño
plt.tight_layout()

# Mostrar el gráfico
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Supongamos que tienes un DataFrame llamado 'df'
# Asegúrate de tener las bibliotecas y el DataFrame correctamente cargados antes de ejecutar este código

# df.info()

# Análisis de la clase máster ("y") frente a las características categóricas
fig, axis = plt.subplots(3, 2, figsize=(15, 10))

sns.countplot(ax=axis[0, 0], data=df, x="job", hue="y")
sns.countplot(ax=axis[0, 1], data=df, x="marital", hue="y").set(ylabel=None)
sns.countplot(ax=axis[1, 0], data=df, x="education", hue="y").set(ylabel=None)
sns.countplot(ax=axis[1, 1], data=df, x="default", hue="y").set(ylabel=None)
sns.countplot(ax=axis[2, 0], data=df, x="housing", hue="y")
sns.countplot(ax=axis[2, 1], data=df, x="loan", hue="y").set(ylabel=None)
#falta poutcome

plt.tight_layout()

plt.show()

"""Combinaciones de la clase con varias predictoras"""

df.info()

fig, axis = plt.subplots(figsize = (10, 5), ncols = 2)

sns.barplot(ax = axis[0], data = df, x = "age", y = "job", hue = "y")
sns.barplot(ax = axis[1], data = df, x = "emp.var.rate", y = "cons.conf.idx", hue = "y").set(ylabel = None)

plt.tight_layout()

plt.show()

"""Análisis de correlaciones"""

df.info()

df["age"] = pd.factorize(df["age"])[0]
df["y"] = pd.factorize(df["y"])[0]

fig, axis = plt.subplots(figsize = (10, 6))

sns.heatmap(df[["job", "marital", "education", "default", "housing", "loan", "poutcome", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m"]].corr(), annot = True, fmt = ".2f")

plt.tight_layout()

plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Supongamos que ya has realizado la factorización de las variables "age" y "y"
# df["age"] = pd.factorize(df["age"])[0]
# df["y"] = pd.factorize(df["y"])[0]

# Crear un DataFrame con las variables de interés
selected_columns = ["age", "job", "marital", "education", "default", "housing", "loan", "poutcome", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m", "y"]
df_selected = df[selected_columns]

# Calcular la matriz de correlación de punto biserial
correlation_matrix = df_selected.corr()

# Crear un mapa de calor
fig, axis = plt.subplots(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm")

plt.tight_layout()
plt.show()

# Seleccionar las columnas de interés
selected_columns = ["age", "job", "marital", "education", "default", "housing", "loan", "poutcome", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m", "y"]
df_selected = df[selected_columns]

# Calcular la correlación numérica
correlation_matrix = df_selected.corr()

# Mostrar la matriz de correlación
print(correlation_matrix)

"""Interpretación de la correlación:

✅age vs. emp.var.rate: La correlación es aproximadamente -0.15. Indica una correlación negativa débil entre la edad y la tasa de variación del empleo. Esto podría sugerir que las personas más jóvenes tienden a experimentar mayores variaciones en las tasas de empleo.

✅age vs. euribor3m: La correlación es aproximadamente -0.16. Indica una correlación negativa débil entre la edad y la tasa Euribor a 3 meses. Puede significar que las personas más jóvenes podrían tener tasas Euribor más altas.

✅emp.var.rate vs. euribor3m: La correlación es alta y positiva, alrededor de 0.97. Esto es esperado, ya que la tasa de variación del empleo (emp.var.rate) y la tasa Euribor a 3 meses (euribor3m) están relacionadas en el contexto económico.

✅emp.var.rate vs. y: La correlación es aproximadamente -0.30. Indica una correlación moderadamente negativa entre la tasa de variación del empleo y la variable objetivo "y". Esto sugiere que a medida que la tasa de variación del empleo disminuye, es más probable que la variable objetivo "y" sea positiva (indicando la realización de un depósito a largo plazo).

✅euribor3m vs. y: La correlación es aproximadamente -0.31. Similar a la correlación anterior, sugiere que a medida que la tasa Euribor a 3 meses disminuye, es más probable que la variable objetivo "y" sea positiva.
"""



"""Foco por tanto en:
- emp.var.rate vs. euribor3m:
- cons.price.idx vs emp.var.rate
"""

fig, axis = plt.subplots(figsize = (10, 5), ncols = 2)

sns.regplot(ax = axis[0], data = df, x = "emp.var.rate", y = "euribor3m")
sns.regplot(ax = axis[1], data = df, x = "cons.price.idx", y = "emp.var.rate").set(ylabel = None, ylim = (0.9, 3.1))

plt.tight_layout()

plt.show()

"""Todos juntos

"""

sns.pairplot(data = df)

"""Feature engineering"""

#outliers
df.describe()

#pintamos los boxplots
fig, axis = plt.subplots(3, 3, figsize = (15, 10))

sns.boxplot(ax = axis[0, 0], data = df, y = "y")
sns.boxplot(ax = axis[0, 1], data = df, y = "age")
sns.boxplot(ax = axis[0, 2], data = df, y = "campaign")
sns.boxplot(ax = axis[1, 0], data = df, y = "emp.var.rate")
sns.boxplot(ax = axis[1, 1], data = df, y = "cons.price.idx")
sns.boxplot(ax = axis[1, 2], data = df, y = "cons.conf.idx")
sns.boxplot(ax = axis[2, 0], data = df, y = "euribor3m")

plt.tight_layout()

plt.show()

# Variables de interés
variables_interes = ["y", "age", "campaign", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m"]

# Obtener estadísticas descriptivas para cada variable
for variable in variables_interes:
    variable_stats = df[variable].describe()
    print(f"\nEstadísticas descriptivas para {variable}:\n{variable_stats}")

# Variables de interés
variables_interes = ["y", "campaign", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m"]

# Calcular límites superior e inferior para la búsqueda de outliers para cada variable
for variable in variables_interes:
    variable_stats = df[variable].describe()

    iqr = variable_stats["75%"] - variable_stats["25%"]
    upper_limit = variable_stats["75%"] + 1.5 * iqr
    lower_limit = variable_stats["25%"] - 1.5 * iqr

    print(f"\nLímites superior e inferior para la búsqueda de outliers de {variable}:")
    print(f"Superior: {round(upper_limit, 2)}, Inferior: {round(lower_limit, 2)}, Rango intercuartílico: {round(iqr, 2)}")

"""¿Sumar o restar el Rango intercuartílico a los límites?


Límite superior: 6.0
Límite inferior: -2.0
Rango intercuartílico: 2.0
Variable "emp.var.rate":

Límite superior: 6.2
Límite inferior: -6.6
Rango intercuartílico: 3.2
Variable "cons.price.idx":

Límite superior: 95.37
Límite inferior: 91.7
Rango intercuartílico: 0.92
Variable "cons.conf.idx":

Límite superior: -26.95
Límite inferior: -52.15
Rango intercuartílico: 6.3
Variable "euribor3m":

Límite superior: 10.39
Límite inferior: -4.08
Rango intercuartílico: 3.62

Una vez aclarado este tema, eliminar outlier (valores atipicos)
"""

df.shape
print("before")
import pandas as pd

# Definir los límites para cada variable
limits = {
    "y": (None, None),  # No hay límites para "y"
    "campaign": (-2.0, 6.0),
    "emp.var.rate": (-6.6, 6.2),
    "cons.price.idx": (91.7, 95.37),
    "cons.conf.idx": (-52.15, -26.95),
    "euribor3m": (-4.08, 10.39)
}

# Iterar a través de cada variable y aplicar los límites para eliminar outliers
for column, (lower_limit, upper_limit) in limits.items():
    if lower_limit is not None:
        df = df[df[column] > lower_limit]
    if upper_limit is not None:
        df = df[df[column] < upper_limit]

# Ver el DataFrame resultante sin outliers
print(df)
print("after")

df.shape

#Eliminación de valores nulos
valores_nulos = df.isnull().sum().sort_values(ascending=False)
print(valores_nulos, "No hay valores nulos")

"""Inferencia de nuevas características no parece aplicar

Escalado de valores -> Normalización: Es una técnica que cambia el rango de los valores de los datos para que puedan ser comparables entre sí. El escalado normalmente implica la normalización, que es el proceso de cambiar los valores para que tengan una media de 0 y una desviación estándar de 1

Normalización
"""

from sklearn.preprocessing import StandardScaler
import pandas as pd

# Supongamos que tienes un DataFrame llamado 'df' con las variables de interés
num_variables = ["age", "campaign", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m"]

scaler = StandardScaler()
norm_features = scaler.fit_transform(df[num_variables])
df_norm = pd.DataFrame(norm_features, index=df.index, columns=num_variables)
df_norm["y"] = df["y"]  # Variable objetivo
df_norm.head()

"""Escalado Mínimo-Máximo (aqui no aplica)"""

'''from sklearn.preprocessing import MinMaxScaler
import pandas as pd

# Supongamos que tienes un DataFrame llamado 'df' con las variables de interés
num_variables = ["age", "campaign", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m"]

scaler = MinMaxScaler()
scaled_features = scaler.fit_transform(df[num_variables])
df_scaled = pd.DataFrame(scaled_features, index=df.index, columns=num_variables)
df_scaled["y"] = df["y"]  # Asegúrate de incluir tu variable objetivo si es necesario
df_scaled.head()'''

"""Feature Selection (el modelo al final es min max)"""

from sklearn.feature_selection import chi2, SelectKBest
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import pandas as pd

# Assuming you have already defined your DataFrame df

# Encode categorical variables
le = LabelEncoder()
df_encoded = df.apply(lambda x: le.fit_transform(x) if x.dtype == 'O' else x)

# Divide the dataset into training and test samples.
X = df_encoded.drop("y", axis=1)
y = df_encoded["y"]

# Scale the features to [0, 1] range
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Use random_state to ensure reproducibility
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# With a value of k = 5, we implicitly mean that we want to remove 2 features from the dataset
selection_model = SelectKBest(chi2, k=5)
selection_model.fit(X_train, y_train)

# Get the indices of the selected features
ix = selection_model.get_support()

# Extract the selected features for both training and test sets
X_train_sel = pd.DataFrame(selection_model.transform(X_train), columns=X.columns.values[ix])
X_test_sel = pd.DataFrame(selection_model.transform(X_test), columns=X.columns.values[ix])

# Display the selected features
print(X_train_sel.head())

X_test_sel.head()

X_train_sel["y"] = list(y_train)
X_test_sel["y"] = list(y_test)

X_train_sel.to_csv("clean-bank-marketing-campaign-data.train.csv", index=False)
X_test_sel.to_csv("clean-bank-marketing-campaign-data.test.csv", index=False)

# Guardar los DataFrames en archivos CSV
X_train_sel.to_csv("clean-bank-marketing-campaign-data.train.csv", index=False)
X_test_sel.to_csv("clean-bank-marketing-campaign-data.test.csv", index=False)

"""Construcción de modelo de regresión logistica"""

#importación de datos
import pandas as pd

train_data = pd.read_csv("clean-bank-marketing-campaign-data.train.csv")
test_data = pd.read_csv("clean-bank-marketing-campaign-data.test.csv")

train_data.head()

"""Optimización del modelo"""

#Paso 1 división de variables predictoras según modelos de testeo y modelo de entrenamiento
X_train = train_data.drop(["y"], axis = 1)
y_train = train_data["y"]
X_test = test_data.drop(["y"], axis = 1)
y_test = test_data["y"]

#Paso 2: Inicialización y entrenamiento del modelo
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

# Divide el conjunto de datos en conjuntos de entrenamiento y prueba con random_state
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Inicializa el modelo con random_state
model = LogisticRegression(random_state=42)

# Entrena el modelo
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_pred

from sklearn.metrics import accuracy_score

accuracy_score(y_test, y_pred)

from sklearn.metrics import accuracy_score

accuracy_score(y_test, y_pred)

# Calcula la precisión y conviértela a porcentaje
accuracy = accuracy_score(y_test, y_pred)
accuracy_porcentaje = accuracy * 100

print(f'La Precisión del modelo es de: {accuracy_porcentaje:.2f}%')

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

mkt_cm = confusion_matrix(y_test, y_pred)

# Dibujaremos esta matriz para hacerla más visual
mkt_df = pd.DataFrame(mkt_cm)

plt.figure(figsize = (3, 3))
sns.heatmap(cm_df, annot=True, fmt="d", cbar=False)

plt.tight_layout()

plt.show()

"""La interpretación de una matriz de confusión es la siguiente:

✅Verdaderos positivos (TP, True positive): Se corresponde con el número 6493 y son los casos en los que el modelo predijo positivo (no supervivencia) y la clase real también es positiva.
✅Verdaderos negativos (TN, False negative): Se corresponde con el número 211 y son los casos en los que el modelo predijo negativo (supervivencia) y la clase real también es negativa.
⛔Falsos positivos (FP, False positive): Se corresponde con el número 645 y son los casos en los que el modelo predijo positivo y la clase real es negativa.
⛔Falsos negativos (FN, False negative): Se corresponde con el número 121 y son los casos en los que el modelo predijo negativo y la clase real es positiva.
Estas cuatro medidas se utilizan a menudo para calcular métricas más complejas
"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Calcular la matriz de confusión
cm = confusion_matrix(y_test, y_pred)

# Definir métricas
tp, fn, fp, tn = cm.ravel()

# Etiquetas y valores para el gráfico
labels = ['Verdaderos positivos', 'Falsos negativos', 'Falsos positivos', 'Verdaderos negativos']
values = [tp, fn, fp, tn]

# Crear un gráfico de barras
plt.figure(figsize=(8, 6))
sns.barplot(x=labels, y=values, palette='viridis')
plt.title('Métricas de la Matriz de Confusión')
plt.ylabel('Cantidad')
plt.show()

"""Optimización de hiperparámetros"""

from sklearn.model_selection import GridSearchCV

# Definimos los parámetros a mano que queremos ajustar
hyperparams = {
    "C": [0.001, 0.01, 0.1, 1, 10, 100, 1000],
    "penalty": ["l1", "l2", "elasticnet", None],
    "solver": ["newton-cg", "lbfgs", "liblinear", "sag", "saga"]
}

# Inicializamos la grid
grid = GridSearchCV(model, hyperparams, scoring = "accuracy", cv = 5)
grid

"""Mejora con grid search"""

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

# Divide el conjunto de datos en conjuntos de entrenamiento y prueba con random_state
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Inicializa el modelo base con random_state
base_model = LogisticRegression(random_state=42)

# Definimos los parámetros a mano que queremos ajustar
hyperparams = {
    "C": [0.001, 0.01, 0.1, 1, 10, 100, 1000],
    "penalty": ["l1", "l2", "elasticnet", None],
    "solver": ["newton-cg", "lbfgs", "liblinear", "sag", "saga"]
}

# Inicializa la cuadrícula con random_state
grid = GridSearchCV(base_model, hyperparams, scoring="accuracy", cv=5)

def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn

grid.fit(X_train, y_train)

print(f"Mejores hiperparámetros: {grid.best_params_}")

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

# Divide el conjunto de datos en conjuntos de entrenamiento y prueba con random_state
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Crea y entrena el modelo con parámetros corregidos y random_state
model_grid = LogisticRegression(penalty='l2', C=100, solver="sag", random_state=42)
model_grid.fit(X_train, y_train)

# Realiza predicciones en el conjunto de prueba
y_pred = model_grid.predict(X_test)

# Calcula la precisión
grid_accuracy = accuracy_score(y_test, y_pred)
print("Precisión del modelo:", grid_accuracy)

"""Mejora"""

# Valores originales
original_valor = 0.9077643908969211
nuevo_valor = 0.9085676037483267

# Calcular la mejora porcentual
mejora_porcentaje = ((nuevo_valor - original_valor) / original_valor) * 100

print("Mejora porcentual:", mejora_porcentaje)

"""Mejora Aleatoria"""

import numpy as np
from sklearn.model_selection import RandomizedSearchCV

# Definimos los parámetros que queremos ajustar
hyperparams = {
    "C": np.logspace(-4, 4, 20),
    "penalty": ["l1", "l2", "elasticnet", None],
    "solver": ["newton-cg", "lbfgs", "liblinear", "sag", "saga"]
}

# Inicializamos la búsqueda aleatoria
random_search = RandomizedSearchCV(model, hyperparams, n_iter = 100, scoring = "accuracy", cv = 5, random_state = 42)
random_search

random_search.fit(X_train, y_train)

print(f"Mejores hiperparámetros: {random_search.best_params_}")

"""Reentrenamiento del modelo"""

model_random_search = LogisticRegression(penalty = "l2", C = 29.7635, solver = "lbfgs")
model_random_search.fit(X_train, y_train)
y_pred = model_random_search.predict(X_test)

random_search_accuracy = accuracy_score(y_test, y_pred)
random_search_accuracy

# Valores originales
original_valor = 0.9077643908969211
nuevo_valor = 0.9085676037483267


# Calcular la mejora porcentual
mejora_porcentaje = ((nuevo_valor - original_valor) / original_valor) * 100

print("Mejora porcentual:", mejora_porcentaje)