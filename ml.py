# -*- coding: utf-8 -*-
"""ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PT6cqyEGFZ6rArfoBIu2EFQ2HxnsAO6j
"""

import pandas as pd
# https://4geeks.com/syllabus/spain-ds-pt-3/read/exploratory-data-analysis-and-cleaning-with-titanic
# https://4geeks.com/syllabus/spain-ds-pt-3/project/logistic-regression-project-tutorial

url = "https://raw.githubusercontent.com/4GeeksAcademy/logistic-regression-project-tutorial/main/bank-marketing-campaign-data.csv"
df = pd.read_csv(url)

df = pd.read_csv(url, sep=';')

df.info()

df.shape

print(df.isnull().sum())

print(df.duplicated().sum())

duplicates = df[df.duplicated()]

# Show duplicate rows
print("Duplicate rows:")
print(duplicates)

duplicates_count = df[df.duplicated(keep=False)].value_counts()

# Show duplicate count
print("Duplicate values count:")
print(duplicates_count)

df.drop_duplicates(inplace=True)

df.shape

df.drop(["contact", "month", "day_of_week", "pdays", "previous", "nr.employed"], axis=1, inplace=True)

df.info()

"""Categorical Variables"""

import matplotlib.pyplot as plt
import seaborn as sns

# Create a figure and axes object with subplots 2x4
fig, axes = plt.subplots(2, 4, figsize=(20, 10))

# Create histograms for each variable
sns.histplot(data=df, x="age", ax=axes[0, 0]).set_xlim(0, 99)
sns.countplot(data=df, x="job", ax=axes[0, 1]).set(ylabel=None)
sns.countplot(data=df, x="marital", ax=axes[0, 2]).set(ylabel=None)
sns.countplot(data=df, x="education", ax=axes[0, 3]).set(ylabel=None)
sns.countplot(data=df, x="default", ax=axes[1, 0]).set(ylabel=None)
sns.countplot(data=df, x="housing", ax=axes[1, 1]).set(ylabel=None)

# Add 'loan' and 'poutcome' variables
sns.countplot(data=df, x="loan", ax=axes[1, 2]).set(ylabel=None)
sns.countplot(data=df, x="poutcome", ax=axes[1, 3]).set(ylabel=None)

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()

"""Numeric Variables"""

import matplotlib.pyplot as plt
import seaborn as sns

# Create a figure and axes object with subplots 4x4
fig, axes = plt.subplots(4, 4, figsize=(10, 7))

# Create histograms and boxplots for numeric variables
sns.histplot(ax=axes[0, 0], data=df, x="age").set(xlabel=None)
sns.boxplot(ax=axes[1, 0], data=df, x="age")
sns.histplot(ax=axes[0, 1], data=df, x="campaign").set(xlabel=None, ylabel=None)
sns.boxplot(ax=axes[1, 1], data=df, x="campaign")
sns.histplot(ax=axes[0, 2], data=df, x="emp.var.rate").set(xlabel=None, ylabel=None)
sns.boxplot(ax=axes[1, 2], data=df, x="emp.var.rate")
sns.histplot(ax=axes[0, 3], data=df, x="cons.price.idx").set(xlabel=None)
sns.boxplot(ax=axes[1, 3], data=df, x="cons.price.idx")

sns.histplot(ax=axes[2, 0], data=df, x="cons.conf.idx").set(xlabel=None, ylabel=None)
sns.boxplot(ax=axes[3, 0], data=df, x="cons.conf.idx")
sns.histplot(ax=axes[2, 1], data=df, x="euribor3m").set(xlabel=None, ylabel=None)
sns.boxplot(ax=axes[3, 1], data=df, x="euribor3m")
sns.histplot(ax=axes[2, 2], data=df, x="duration").set(xlabel=None, ylabel=None)
sns.boxplot(ax=axes[3, 2], data=df, x="duration")

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()

"""Multivariables"""

import matplotlib.pyplot as plt
import seaborn as sns

fig, axis = plt.subplots(2, 2, figsize=(10, 7))

# Create a scatter plot
sns.regplot(ax=axis[0, 0], data=df, x="age", y="campaign")
sns.heatmap(df[["age", "campaign"]].corr(), annot=True, fmt=".2f", ax=axis[1, 0], cbar=False)

# You can replace "emp.var.rate" and "cons.price.idx" with the variables of interest
sns.regplot(ax=axis[0, 1], data=df, x="emp.var.rate", y="cons.price.idx").set(ylabel=None)
sns.heatmap(df[["emp.var.rate", "cons.price.idx"]].corr(), annot=True, fmt=".2f", ax=axis[1, 1])

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Assume you have a DataFrame named 'df'
# Make sure to have the libraries and DataFrame correctly loaded before running this code

# df.info()

# Analysis of the master class ("y") against categorical features
fig, axis = plt.subplots(3, 2, figsize=(15, 10))

sns.countplot(ax=axis[0, 0], data=df, x="job", hue="y")
sns.countplot(ax=axis[0, 1], data=df, x="marital", hue="y").set(ylabel=None)
sns.countplot(ax=axis[1, 0], data=df, x="education", hue="y").set(ylabel=None)
sns.countplot(ax=axis[1, 1], data=df, x="default", hue="y").set(ylabel=None)
sns.countplot(ax=axis[2, 0], data=df, x="housing", hue="y")
sns.countplot(ax=axis[2, 1], data=df, x="loan", hue="y").set(ylabel=None)
# Missing poutcome

plt.tight_layout()

plt.show()

"""Combinations of the class with multiple predictors"""

df.info()

fig, axis = plt.subplots(figsize=(10, 5), ncols=2)

sns.barplot(ax=axis[0], data=df, x="age", y="job", hue="y")
sns.barplot(ax=axis[1], data=df, x="emp.var.rate", y="cons.conf.idx", hue="y").set(ylabel=None)

plt.tight_layout()

plt.show()

"""Correlation Analysis"""

df.info()

df["age"] = pd.factorize(df["age"])[0]
df["y"] = pd.factorize(df["y"])[0]

fig, axis = plt.subplots(figsize=(10, 6))

sns.heatmap(df[["job", "marital", "education", "default", "housing", "loan", "poutcome", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m"]].corr(), annot=True, fmt=".2f")

plt.tight_layout()

plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Suppose you have already factorized the variables "age" and "y"
# df["age"] = pd.factorize(df["age"])[0]
# df["y"] = pd.factorize(df["y"])[0]

# Create a DataFrame with the variables of interest
selected_columns = ["age", "job", "marital", "education", "default", "housing", "loan", "poutcome", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m", "y"]
df_selected = df[selected_columns]

# Calculate point biserial correlation matrix
correlation_matrix = df_selected.corr()

# Create a heatmap
fig, axis = plt.subplots(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm")

plt.tight_layout()
plt.show()

# Select the columns of interest
selected_columns = ["age", "job", "marital", "education", "default", "housing", "loan", "poutcome", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m", "y"]
df_selected = df[selected_columns]

# Calculate numeric correlation
correlation_matrix = df_selected.corr()

# Show the correlation matrix
print(correlation_matrix)

"""Interpretation of correlation:

✅age vs. emp.var.rate: The correlation is approximately -0.15. It indicates a weak negative correlation between age and the employment variation rate. This could suggest that younger people tend to experience higher variations in employment rates.

✅age vs. euribor3m: The correlation is approximately -0.16. It indicates a weak negative correlation between age and the Euribor 3-month rate. It may mean that younger people might have higher Euribor rates.

✅emp.var.rate vs. euribor3m: The correlation is high and positive, around 0.97. This is expected as the employment variation rate (emp.var.rate) and the Euribor 3-month rate (euribor3m) are related in the economic context.

✅emp.var.rate vs. y: The correlation is approximately -0.30. It indicates a moderately negative correlation between the employment variation rate and the target variable "y." This suggests that as the employment variation rate decreases, it is more likely that the target variable "y" is positive (indicating a long-term deposit).

✅euribor3m vs. y: The correlation is approximately -0.31. Similar to the previous correlation, it suggests that as the Euribor 3-month rate decreases, it is more likely that the target variable "y" is positive.
"""

"""Focus on:
- emp.var.rate vs. euribor3m:
- cons.price.idx vs. emp.var.rate
"""

fig, axis = plt.subplots(figsize=(10, 5), ncols=2)

sns.regplot(ax=axis[0], data=df, x="emp.var.rate", y="euribor3m")
sns.regplot(ax=axis[1], data=df, x="cons.price.idx", y="emp.var.rate").set(ylabel=None, ylim=(0.9, 3.1))

plt.tight_layout()

plt.show()

"""All Together"""

sns.pairplot(data=df)

"""Feature Engineering"""

# outliers
df.describe()

# plot boxplots
fig, axis = plt.subplots(3, 3, figsize=(15, 10))

sns.boxplot(ax=axis[0, 0], data=df, y="y")
sns.boxplot(ax=axis[0, 1], data=df, y="age")
sns.boxplot(ax=axis[0, 2], data=df, y="campaign")
sns.boxplot(ax=axis[1, 0], data=df, y="emp.var.rate")
sns.boxplot(ax=axis[1, 1], data=df, y="cons.price.idx")
sns.boxplot(ax=axis[1, 2], data=df, y="cons.conf.idx")
sns.boxplot(ax=axis[2, 0], data=df, y="euribor3m")

plt.tight_layout()

plt.show()

# Variables of interest
variables_interest = ["y", "age", "campaign", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m"]

# Get descriptive statistics for each variable
for variable in variables_interest:
    variable_stats = df[variable].describe()
    print(f"\nDescriptive statistics for {variable}:\n{variable_stats}")

# Variables of interest
variables_interest = ["y", "campaign", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m"]

# Calculate upper and lower limits for outlier search for each variable
for variable in variables_interest:
    variable_stats = df[variable].describe()

    iqr = variable_stats["75%"] - variable_stats["25%"]
    upper_limit = variable_stats["75%"] + 1.5 * iqr
    lower_limit = variable_stats["25%"] - 1.5 * iqr

    print(f"\nUpper and lower limits for outlier search of {variable}:")
    print(f"Upper: {round(upper_limit, 2)}, Lower: {round(lower_limit, 2)}, Interquartile range: {round(iqr, 2)}")

"""¿Sumar o restar el Rango intercuartílico a los límites?

Límite superior: 6.0
Límite inferior: -2.0
Rango intercuartílico: 2.0
Variable "emp.var.rate":

Límite superior: 6.2
Límite inferior: -6.6
Rango intercuartílico: 3.2
Variable "cons.price.idx":

Límite superior: 95.37
Límite inferior: 91.7
Rango intercuartílico: 0.92
Variable "cons.conf.idx":

Límite superior: -26.95
Límite inferior: -52.15
Rango intercuartílico: 6.3
Variable "euribor3m":

Límite superior: 10.39
Límite inferior: -4.08
Rango intercuartílico: 3.62

Una vez aclarado este tema, eliminar outlier (valores atipicos)
"""

df.shape
print("before")
import pandas as pd

# Define limits for each variable
limits = {
    "y": (None, None),  # No hay límites para "y"
    "campaign": (-2.0, 6.0),
    "emp.var.rate": (-6.6, 6.2),
    "cons.price.idx": (91.7, 95.37),
    "cons.conf.idx": (-52.15, -26.95),
    "euribor3m": (-4.08, 10.39)
}

# Iterate through each variable and apply limits to remove outliers
for column, (lower_limit, upper_limit) in limits.items():
    if lower_limit is not None:
        df = df[df[column] > lower_limit]
    if upper_limit is not None:
        df = df[df[column] < upper_limit]

# View the resulting DataFrame without outliers
print(df)
print("after")

df.shape

#Delete null values
valores_nulos = df.isnull().sum().sort_values(ascending=False)
print(valores_nulos, "No hay valores nulos")

"""Inferencia de nuevas características no parece aplicar

Escalado de valores -> Normalización: Es una técnica que cambia el rango de los valores de los datos para que puedan ser comparables entre sí. El escalado normalmente implica la normalización, que es el proceso de cambiar los valores para que tengan una media de 0 y una desviación estándar de 1

Normalización
"""
from sklearn.preprocessing import StandardScaler
import pandas as pd

num_variables = ["age", "campaign", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m"]

scaler = StandardScaler()
norm_features = scaler.fit_transform(df[num_variables])
df_norm = pd.DataFrame(norm_features, index=df.index, columns=num_variables)
df_norm["y"] = df["y"]  # Variable objetivo
df_norm.head()

"""Escalado Mínimo-Máximo (si aplica)"""

'''from sklearn.preprocessing import MinMaxScaler
import pandas as pd

# Supongamos que tienes un DataFrame llamado 'df' con las variables de interés
num_variables = ["age", "campaign", "emp.var.rate", "cons.price.idx", "cons.conf.idx", "euribor3m"]

scaler = MinMaxScaler()
scaled_features = scaler.fit_transform(df[num_variables])
df_scaled = pd.DataFrame(scaled_features, index=df.index, columns=num_variables)
df_scaled["y"] = df["y"]  # Asegúrate de incluir tu variable objetivo si es necesario
df_scaled.head()'''

"""Feature Selection (el modelo al final es min max)"""

from sklearn.feature_selection import chi2, SelectKBest
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import pandas as pd

# Assuming you have already defined your DataFrame df

# Encode categorical variables
le = LabelEncoder()
df_encoded = df.apply(lambda x: le.fit_transform(x) if x.dtype == 'O' else x)

# Divide the dataset into training and test samples.
X = df_encoded.drop("y", axis=1)
y = df_encoded["y"]

# Scale the features to [0, 1] range
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Use random_state to ensure reproducibility
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# With a value of k = 5, we implicitly mean that we want to remove 2 features from the dataset
selection_model = SelectKBest(chi2, k=5)
selection_model.fit(X_train, y_train)

# Get the indices of the selected features
ix = selection_model.get_support()

# Extract the selected features for both training and test sets
X_train_sel = pd.DataFrame(selection_model.transform(X_train), columns=X.columns.values[ix])
X_test_sel = pd.DataFrame(selection_model.transform(X_test), columns=X.columns.values[ix])

# Display the selected features
print(X_train_sel.head())

X_test_sel.head()

X_train_sel["y"] = list(y_train)
X_test_sel["y"] = list(y_test)

X_train_sel.to_csv("clean-bank-marketing-campaign-data.train.csv", index=False)
X_test_sel.to_csv("clean-bank-marketing-campaign-data.test.csv", index=False)

# Guardar los DataFrames en archivos CSV
X_train_sel.to_csv("clean-bank-marketing-campaign-data.train.csv", index=False)
X_test_sel.to_csv("clean-bank-marketing-campaign-data.test.csv", index=False)

# Importación de datos
import pandas as pd

train_data = pd.read_csv("clean-bank-marketing-campaign-data.train.csv")
test_data = pd.read_csv("clean-bank-marketing-campaign-data.test.csv")

train_data.head()

"""Optimización del modelo"""

# Paso 1: División de variables predictoras según modelos de prueba y entrenamiento
X_train = train_data.drop(["y"], axis=1)
y_train = train_data["y"]
X_test = test_data.drop(["y"], axis=1)
y_test = test_data["y"]

# Paso 2: Inicialización y entrenamiento del modelo
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

# Divide el conjunto de datos en conjuntos de entrenamiento y prueba con random_state
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Inicializa el modelo con random_state
model = LogisticRegression(random_state=42)

# Entrena el modelo
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_pred

from sklearn.metrics import accuracy_score

accuracy_score(y_test, y_pred)

from sklearn.metrics import accuracy_score

accuracy_score(y_test, y_pred)

# Calcula la precisión y conviértela a porcentaje
accuracy = accuracy_score(y_test, y_pred)
accuracy_porcentaje = accuracy * 100

print(f'La Precisión del modelo es de: {accuracy_porcentaje:.2f}%')

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

mkt_cm = confusion_matrix(y_test, y_pred)

# Dibujaremos esta matriz para hacerla más visual
mkt_df = pd.DataFrame(mkt_cm)

plt.figure(figsize=(3, 3))
sns.heatmap(cm_df, annot=True, fmt="d", cbar=False)

plt.tight_layout()

plt.show()

'''The interpretation of a confusion matrix is as follows:

✅ True Positives (TP, True positive): Corresponds to the number 6493, indicating cases where the model predicted positive (non-survival), and the actual class is also positive.
✅ True Negatives (TN, False negative): Corresponds to the number 211, indicating cases where the model predicted negative (survival), and the actual class is also negative.
⛔ False Positives (FP, False positive): Corresponds to the number 645, indicating cases where the model predicted positive, but the actual class is negative.
⛔ False Negatives (FN, False negative): Corresponds to the number 121, indicating cases where the model predicted negative, but the actual class is positive.
These four measures are often used to calculate more complex metrics.'''

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Calculate confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Definir métricas
tp, fn, fp, tn = cm.ravel()

# Etiquetas y valores para el gráfico
labels = ['Verdaderos positivos', 'Falsos negativos', 'Falsos positivos', 'Verdaderos negativos']
values = [tp, fn, fp, tn]

# Crear un gráfico de barras
plt.figure(figsize=(8, 6))
sns.barplot(x=labels, y=values, palette='viridis')
plt.title('Métricas de la Matriz de Confusión')
plt.ylabel('Cantidad')
plt.show()

"""Optimización de hiperparámetros"""

from sklearn.model_selection import GridSearchCV

# Definimos los parámetros a mano que queremos ajustar
hyperparams = {
    "C": [0.001, 0.01, 0.1, 1, 10, 100, 1000],
    "penalty": ["l1", "l2", "elasticnet", None],
    "solver": ["newton-cg", "lbfgs", "liblinear", "sag", "saga"]
}

# Inicializamos la grid
grid = GridSearchCV(model, hyperparams, scoring = "accuracy", cv = 5)
grid

"""Mejora con grid search"""

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

# Divide el conjunto de datos en conjuntos de entrenamiento y prueba con random_state
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Inicializa el modelo base con random_state
base_model = LogisticRegression(random_state=42)

# Definimos los parámetros a mano que queremos ajustar
hyperparams = {
    "C": [0.001, 0.01, 0.1, 1, 10, 100, 1000],
    "penalty": ["l1", "l2", "elasticnet", None],
    "solver": ["newton-cg", "lbfgs", "liblinear", "sag", "saga"]
}

# Inicializa la cuadrícula con random_state
grid = GridSearchCV(base_model, hyperparams, scoring="accuracy", cv=5)

def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn

grid.fit(X_train, y_train)

print(f"Mejores hiperparámetros: {grid.best_params_}")

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

# Divide el conjunto de datos en conjuntos de entrenamiento y prueba con random_state
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Crea y entrena el modelo con parámetros corregidos y random_state
model_grid = LogisticRegression(penalty='l2', C=100, solver="sag", random_state=42)
model_grid.fit(X_train, y_train)

# Realiza predicciones en el conjunto de prueba
y_pred = model_grid.predict(X_test)

# Calcula la precisión
grid_accuracy = accuracy_score(y_test, y_pred)
print("Precisión del modelo:", grid_accuracy)

"""Mejora"""

# Valores originales
original_valor = 0.9077643908969211
nuevo_valor = 0.9085676037483267

# Calcular la mejora porcentual
mejora_porcentaje = ((nuevo_valor - original_valor) / original_valor) * 100

print("Mejora porcentual:", mejora_porcentaje)

"""Mejora Aleatoria"""

import numpy as np
from sklearn.model_selection import RandomizedSearchCV

# Definimos los parámetros que queremos ajustar
hyperparams = {
    "C": np.logspace(-4, 4, 20),
    "penalty": ["l1", "l2", "elasticnet", None],
    "solver": ["newton-cg", "lbfgs", "liblinear", "sag", "saga"]
}

# Inicializamos la búsqueda aleatoria
random_search = RandomizedSearchCV(model, hyperparams, n_iter = 100, scoring = "accuracy", cv = 5, random_state = 42)
random_search

random_search.fit(X_train, y_train)

print(f"Mejores hiperparámetros: {random_search.best_params_}")

"""Reentrenamiento del modelo"""

model_random_search = LogisticRegression(penalty = "l2", C = 29.7635, solver = "lbfgs")
model_random_search.fit(X_train, y_train)
y_pred = model_random_search.predict(X_test)

random_search_accuracy = accuracy_score(y_test, y_pred)
random_search_accuracy

# Valores originales
original_valor = 0.9077643908969211
nuevo_valor = 0.9085676037483267


# Calcular la mejora porcentual
mejora_porcentaje = ((nuevo_valor - original_valor) / original_valor) * 100

print("Mejora porcentual:", mejora_porcentaje)